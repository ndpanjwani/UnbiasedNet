{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "833b71b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80885d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 8846\n",
      "\n",
      "Bias distribution:\n",
      "sentence_has_bias\n",
      "0    7268\n",
      "1    1578\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total articles: 300\n"
     ]
    }
   ],
   "source": [
    "# Load CSV\n",
    "df = pd.read_csv('baseline_dataset.csv')\n",
    "\n",
    "sentence_data = df[['sentence_text', 'sentence_has_bias', 'article_id']].copy()\n",
    "\n",
    "print(f\"Total sentences: {len(sentence_data)}\")\n",
    "print(f\"\\nBias distribution:\")\n",
    "print(sentence_data['sentence_has_bias'].value_counts())\n",
    "print(f\"\\nTotal articles: {sentence_data['article_id'].nunique()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f38f2c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>sentence_has_bias</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WASHINGTON — Michael Steele, chairman of the R...</td>\n",
       "      <td>1</td>\n",
       "      <td>nyt_001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“This is not something the United States had a...</td>\n",
       "      <td>0</td>\n",
       "      <td>nyt_001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“It was the president who was trying to be cut...</td>\n",
       "      <td>1</td>\n",
       "      <td>nyt_001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“Well, if he’s such a student of history, has ...</td>\n",
       "      <td>0</td>\n",
       "      <td>nyt_001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mr. Steele, seeking to clarify his remarks, is...</td>\n",
       "      <td>0</td>\n",
       "      <td>nyt_001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       sentence_text  sentence_has_bias  \\\n",
       "0  WASHINGTON — Michael Steele, chairman of the R...                  1   \n",
       "1  “This is not something the United States had a...                  0   \n",
       "2  “It was the president who was trying to be cut...                  1   \n",
       "3  “Well, if he’s such a student of history, has ...                  0   \n",
       "4  Mr. Steele, seeking to clarify his remarks, is...                  0   \n",
       "\n",
       "  article_id  \n",
       "0    nyt_001  \n",
       "1    nyt_001  \n",
       "2    nyt_001  \n",
       "3    nyt_001  \n",
       "4    nyt_001  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fa465e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 7076 sentences (80.0%)\n",
      "Val:   885 sentences (10.0%)\n",
      "Test:  885 sentences (10.0%)\n"
     ]
    }
   ],
   "source": [
    "train_df, temp_df = train_test_split(\n",
    "    sentence_data, \n",
    "    test_size=0.2,\n",
    "    stratify=sentence_data['sentence_has_bias']\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_df['sentence_has_bias'],\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_df)} sentences ({len(train_df)/len(sentence_data)*100:.1f}%)\")\n",
    "print(f\"Val:   {len(val_df)} sentences ({len(val_df)/len(sentence_data)*100:.1f}%)\")\n",
    "print(f\"Test:  {len(test_df)} sentences ({len(test_df)/len(sentence_data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee0d8572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset class defined\n"
     ]
    }
   ],
   "source": [
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        text = row['sentence_text']\n",
    "        label = row['sentence_has_bias']\n",
    "        \n",
    "\n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long),\n",
    "            'article_id': row['article_id']\n",
    "        }\n",
    "\n",
    "print(\"✓ Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef35ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "MODEL_NAME = 'bert-base-uncased'  \n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.00001\n",
    "EPOCHS = 3\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2  \n",
    ").to(device)\n",
    "\n",
    "\n",
    "train_dataset = SentenceDataset(train_df, tokenizer)\n",
    "val_dataset = SentenceDataset(val_df, tokenizer)\n",
    "test_dataset = SentenceDataset(test_df, tokenizer)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "761cd907",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae35f1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f315b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device, verbose=True):\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_article_ids = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label']\n",
    "            article_ids = batch['article_id']\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            # Get predictions\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_article_ids.extend(article_ids.numpy())\n",
    "    \n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n=== Sentence-Level Metrics ===\")\n",
    "        print(f\"F1:        {f1:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall:    {recall:.4f}\")\n",
    "        print(f\"\\nClassification Report:\")\n",
    "        print(classification_report(all_labels, all_preds, target_names=['Neutral', 'Biased']))\n",
    "        \n",
    "        # Compute per-article average bias rate\n",
    "        results_df = pd.DataFrame({\n",
    "            'article_id': all_article_ids,\n",
    "            'prediction': all_preds,\n",
    "            'label': all_labels\n",
    "        })\n",
    "        \n",
    "        article_bias_rate = results_df.groupby('article_id')['prediction'].mean()\n",
    "        print(f\"\\n=== Per-Article Average Bias Rate ===\")\n",
    "        print(f\"Mean bias rate: {article_bias_rate.mean():.4f}\")\n",
    "        print(f\"Std bias rate:  {article_bias_rate.std():.4f}\")\n",
    "        print(f\"Min bias rate:  {article_bias_rate.min():.4f}\")\n",
    "        print(f\"Max bias rate:  {article_bias_rate.max():.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels,\n",
    "        'article_ids': all_article_ids\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96b9c7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 1/3\n",
      "==================================================\n",
      "Train Loss: 0.4273\n",
      "\n",
      "--- Validation ---\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Validation ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m val_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Save best model\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m best_val_f1:\n",
      "Cell \u001b[0;32mIn[13], line 27\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, dataloader, device, verbose)\u001b[0m\n\u001b[1;32m     25\u001b[0m         all_preds\u001b[38;5;241m.\u001b[39mextend(preds\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     26\u001b[0m         all_labels\u001b[38;5;241m.\u001b[39mextend(labels\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m---> 27\u001b[0m         all_article_ids\u001b[38;5;241m.\u001b[39mextend(\u001b[43marticle_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m())\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Compute sentence-level metrics\u001b[39;00m\n\u001b[1;32m     30\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(all_labels, all_preds)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm  # For progress bars\n",
    "\n",
    "best_val_f1 = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Validate\n",
    "    print(\"\\n--- Validation ---\")\n",
    "    val_metrics = evaluate(model, val_loader, device, verbose=True)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_metrics['f1'] > best_val_f1:\n",
    "        best_val_f1 = val_metrics['f1']\n",
    "        torch.save(model.state_dict(), 'best_sentence_model.pt')\n",
    "        print(\"✓ Saved best model\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Training complete!\")\n",
    "print(f\"Best validation F1: {best_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3873bf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_sentence_model.pt'))\n",
    "\n",
    "print(f\"{'='*50}\")\n",
    "print(\"FINAL TEST RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "test_metrics = evaluate(model, test_loader, device, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
